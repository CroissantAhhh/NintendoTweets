{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nintendo Tweets Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because our data science problem is to ask if Textblob performs well on labeling our tweets, our data science approach will be the following:\n",
    "\n",
    "    * Label all of our tweets as \"positive\" or \"negative\"\n",
    "    \n",
    "    * Take samples of each collection of tweets to be evaluated by myself manually\n",
    "    \n",
    "    * Use the rest of the data to train our models\n",
    "    \n",
    "    * Find best performing model, compare its accuracy to my manual labeling\n",
    "    \n",
    "    * Determine if the model's perfomance is acceptable\n",
    "    \n",
    "First I will label all of our data using TextBlob. TextBlob gives any string a sentiment score between -1.0 and 1.0. Here I am choosing to define any nonnegative value as \"positive\" sentiment and any negative value as \"negative\" sentiment. Furthermore I've decided to have the sentiment score of 0 count as positive, because even if the text itself may be neutral, it still means the person cared enough to tweet at all. Positive will be mapped to the value \"1\" while negative will be mapped to the value \"0\".\n",
    "\n",
    "In this phase of the project, I will need to take samples of each of my collection of tweets, one for each game. They will be proportional to the amount of each collection size. For example because Smash Bros has by far the highest amount of tweets, the sample I take for it will have a much larger size compared to the other two games. The samples I take will also preserve their ratio of positive to negative tweets via stratification. It wouldn't help if I randomly took a sample and the sample happens to be all positive tweets. \n",
    "\n",
    "The remaining data will serve as our training data. Although I could vectorize the data right now, it would result in too many separate files. It'll be better to just do this during the beginning of the modeling phase of the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import nltk.sentiment.vader as vd\n",
    "from textblob import TextBlob\n",
    "from wordcloud import WordCloud, STOPWORDS \n",
    "from IPython.display import Image\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/Users/jasonzhou/Documents/GitHub/NintendoTweets/Documents/Capstone3\"\n",
    "os.chdir(path)\n",
    "\n",
    "NintendoTweets = pd.read_json(\"NintendoTweets.json\", lines=True,\n",
    "                        orient='columns')\n",
    "smashdata = pd.read_csv('smashdata.csv')\n",
    "firedata = pd.read_csv('firedata.csv')\n",
    "partydata = pd.read_csv('partydata.csv')\n",
    "\n",
    "NintendoTweets = NintendoTweets['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of Total/Unique Tweets Per Game\n",
      " \n",
      "Super Smash Bros. Ultimate:  12535 / 3016\n",
      "Fire Emblem: Three Houses:  1563 / 200\n",
      "Super Mario Party:  882 / 264\n"
     ]
    }
   ],
   "source": [
    "print(\"Amount of Total/Unique Tweets Per Game\")\n",
    "print(\" \")\n",
    "print(\"Super Smash Bros. Ultimate: \", len(smashdata), \"/\", len(set(smashdata['cleanedtext'])))\n",
    "print(\"Fire Emblem: Three Houses: \", len(firedata), \"/\", len(set(firedata['cleanedtext'])))\n",
    "print(\"Super Mario Party: \", len(partydata), \"/\", len(set(partydata['cleanedtext'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labeling all the data\n",
    "\n",
    "smashlabels = []\n",
    "firelabels = []\n",
    "partylabels = []\n",
    "\n",
    "for i in range(len(smashdata)):\n",
    "    blob = TextBlob(smashdata['cleanedtext'][i])\n",
    "    if blob.sentiment.polarity >= 0:\n",
    "        smashlabels.append(1)\n",
    "    else:\n",
    "        smashlabels.append(0)\n",
    "        \n",
    "for i in range(len(firedata)):\n",
    "    blob = TextBlob(firedata['cleanedtext'][i])\n",
    "    if blob.sentiment.polarity >= 0:\n",
    "        firelabels.append(1)\n",
    "    else:\n",
    "        firelabels.append(0)\n",
    "        \n",
    "for i in range(len(partydata)):\n",
    "    blob = TextBlob(partydata['cleanedtext'][i])\n",
    "    if blob.sentiment.polarity >= 0:\n",
    "        partylabels.append(1)\n",
    "    else:\n",
    "        partylabels.append(0)\n",
    "        \n",
    "#smashdata['label'] = smashlabels\n",
    "#firedata['label'] = firelabels\n",
    "#partydata['label'] = partylabels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When taking samples, I want to see the original, uncleaned tweets because some do not make much sense after being cleaned. I'll also keep the cleaned versions next to their originals in the samples datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  del sys.path[0]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "Xtr1, Xte1, ytr1, yte1 = train_test_split(smashdata, smashlabels, test_size=0.02, random_state=1, stratify = smashlabels)\n",
    "\n",
    "smashoriginaltweets = []\n",
    "for i in range(len(Xte1)):\n",
    "    smashoriginaltweets.append(NintendoTweets[Xte1['Unnamed: 0'].iloc[i]])\n",
    "    \n",
    "smashsampletweets = list(zip(smashoriginaltweets, Xte1['cleanedtext']))\n",
    "smashsamples = pd.DataFrame(smashsampletweets, columns=['Tweet', 'Label'])\n",
    "smashsamples.to_csv(\"smashsamples.csv\")\n",
    "\n",
    "Xtr1['label'] = ytr1\n",
    "Xtr1 = Xtr1[['cleanedtext', 'label']]\n",
    "Xtr1.to_csv(\"smashtraining.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "Xtr2, Xte2, ytr2, yte2 = train_test_split(firedata, firelabels, test_size=0.1, random_state=1, stratify = firelabels)\n",
    "\n",
    "fireoriginaltweets = []\n",
    "for i in range(len(Xte2)):\n",
    "    fireoriginaltweets.append(NintendoTweets[Xte2['Unnamed: 0'].iloc[i]])\n",
    "    \n",
    "firesampletweets = list(zip(fireoriginaltweets, Xte2['cleanedtext']))\n",
    "firesamples = pd.DataFrame(firesampletweets, columns=['Tweet', 'Label'])\n",
    "firesamples.to_csv(\"firesamples.csv\")\n",
    "\n",
    "Xtr2['label'] = ytr2\n",
    "Xtr2 = Xtr2[['cleanedtext', 'label']]\n",
    "Xtr2.to_csv(\"firetraining.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "Xtr3, Xte3, ytr3, yte3 = train_test_split(partydata, partylabels, test_size=0.1, random_state=1, stratify = partylabels)\n",
    "\n",
    "partyoriginaltweets = []\n",
    "for i in range(len(Xte3)):\n",
    "    partyoriginaltweets.append(NintendoTweets[Xte3['Unnamed: 0'].iloc[i]])\n",
    "    \n",
    "partysampletweets = list(zip(partyoriginaltweets, Xte3['cleanedtext']))\n",
    "partysamples = pd.DataFrame(partysampletweets, columns=['Tweet', 'Label'])\n",
    "partysamples.to_csv(\"partysamples.csv\")\n",
    "\n",
    "Xtr3['label'] = ytr3\n",
    "Xtr3 = Xtr3[['cleanedtext', 'label']]\n",
    "Xtr3.to_csv(\"partytraining.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
