{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import statistics\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import datetime\n",
    "import pprint\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "from matplotlib.ticker import IndexLocator\n",
    "import itertools\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading in files\n",
    "\n",
    "path = \"/Users/jasonzhou/Documents/GitHub/NintendoTweets/Documents/Capstone3\"\n",
    "os.chdir(path)\n",
    "\n",
    "smashtraining = pd.read_csv('smashtraining.csv')\n",
    "firetraining = pd.read_csv('firetraining.csv')\n",
    "partytraining = pd.read_csv('partytraining.csv')\n",
    "\n",
    "smashsamples = pd.read_csv('smashsamples.csv')\n",
    "firesamples = pd.read_csv('firesamples.csv')\n",
    "partysamples = pd.read_csv('partysamples.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that creates count vectorizers of corpus, and returns counts array, feature names, \n",
    "# and the vectorizer itself\n",
    "\n",
    "def makeCountVec(df):\n",
    "    vectorizer = CountVectorizer(min_df=0)\n",
    "    vectorizer.fit(df['cleanedtext'])\n",
    "    array = vectorizer.transform(df['cleanedtext'])\n",
    "    array = array.toarray()\n",
    "    features = vectorizer.get_feature_names()\n",
    "    return array, features, vectorizer\n",
    "\n",
    "# classification report function\n",
    "\n",
    "def scores(y_test, y_pred_class):    \n",
    "    target_names = ['positive tweets', 'negative tweets']\n",
    "    print(classification_report(y_test, y_pred_class, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "smashX, smashfeatures, smashvectorizer = makeCountVec(smashtraining)\n",
    "smashy = smashtraining['label']\n",
    "\n",
    "fireX, firefeatures, firevectorizer, = makeCountVec(firetraining)\n",
    "firey = firetraining['label']\n",
    "\n",
    "partyX, partyfeatures, partyvectorizer = makeCountVec(partytraining)\n",
    "partyy = partytraining['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "smashX = smashX[:-len(smashsamples),]\n",
    "fireX = fireX[:-len(firesamples), :]\n",
    "partyX = partyX[:-len(partysamples), :]\n",
    "\n",
    "# Need to do the same for the y vectors too\n",
    "\n",
    "smashy = smashy[:-len(smashsamples)]\n",
    "firey = firey[:-len(firesamples)]\n",
    "partyy = partyy[:-len(partysamples)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "Xtr1, Xte1, ytr1, yte1 = train_test_split(smashX, smashy, test_size=0.3, random_state=1, stratify = smashy)\n",
    "Xtr2, Xte2, ytr2, yte2 = train_test_split(fireX, firey, test_size=0.3, random_state=1, stratify = firey)\n",
    "Xtr3, Xte3, ytr3, yte3 = train_test_split(partyX, partyy, test_size=0.3, random_state=1, stratify = partyy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Importance from Best Tree Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      "positive tweets       1.00      0.68      0.81      3452\n",
      "negative tweets       0.15      0.98      0.26       196\n",
      "\n",
      "       accuracy                           0.69      3648\n",
      "      macro avg       0.57      0.83      0.53      3648\n",
      "   weighted avg       0.95      0.69      0.78      3648\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# XGBClassifier for Smash Bros\n",
    "\n",
    "smashXGB = xgb.XGBClassifier(max_depth=5, learning_rate=0.001, n_estimators=1000, scale_pos_weight=(8052/458))\n",
    "smashXGB.fit(Xtr1, ytr1)\n",
    "ypred = smashXGB.predict(Xte1)\n",
    "\n",
    "scores(yte1, ypred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately our modeling turned out to be a failure. Due to the nature of our data, it was very important for our training data and model to be complex enough to be able to accurately detect the rare negative tweets. In other words the tolerance for our false postive rate was extremely low, for the models to be considered acceptable. The problem is that our bag-of-word representations lack the nuance to truly capture sentiment of the original tweets. Not only this, but TextBlob is most likely not an accurate labeler of our documents (tweets) here either. TextBlob's sentiment analysis is based on a model trained on movie reviews. The language used in movie reviews include many more clear postive or negative words, whereas the language used in tweets are more unfocused and include much more slang. Therefore even a bag-of-word representation of a movie review would still retain enough information to reasonably evaluate sentiment off of. Moving forward, the first step to creating models that accurately label tweets, could be to have a large enough training data set of accurately labeled tweets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
