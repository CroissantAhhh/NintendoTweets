{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nintendo Tweets Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because our data science problem is to ask if Textblob performs well on labeling our tweets, our data science approach will be the following:\n",
    "\n",
    "    * Label all of our tweets as \"positive\" or \"negative\"\n",
    "    \n",
    "    * Take samples of each collection of tweets to be evaluated by myself manually\n",
    "    \n",
    "    * Use the rest of the data to train our models\n",
    "    \n",
    "    * Find best performing model, compare its accuracy to my manual labeling\n",
    "    \n",
    "    * Determine if the model's perfomance is acceptable\n",
    "    \n",
    "First I will label all of our data using TextBlob. TextBlob gives any string a sentiment score between -1.0 and 1.0. Here I am choosing to define any nonnegative value as \"positive\" sentiment and any negative value as \"negative\" sentiment. Furthermore I've decided to have the sentiment score of 0 count as positive, because even if the text itself may be neutral, it still means the person cared enough to tweet at all. \n",
    "\n",
    "Positive will be mapped to the value \"0\" while negative will be mapped to the value \"1\". Why not the other way around? Because the vast majority of tweets are positive and a very small minority are negative. The data is heavily imbalanced. If I built a model that simply always predicted a tweet to be positive without even looking at any of the features, its accuracy would be seemingly quite high. \n",
    "\n",
    "Therefore I want the model to prioritize correctly identifying negative tweets over positive tweets. Because classification report metrics such as precision and recall are based on identifying positives, I believe that setting up the values this way will result in the classification report being more telling.\n",
    "\n",
    "In this phase of the project, I will need to take samples of each of my collection of tweets, one for each game. They will be proportional to the amount of each collection size. For example because Smash Bros has by far the highest amount of tweets, the sample I take for it will have a much larger size compared to the other two games. The samples I take will also preserve their ratio of positive to negative tweets via stratification. It wouldn't help if I randomly took a sample and the sample happens to be all positive tweets. \n",
    "\n",
    "The remaining data will serve as our training data. Although I could vectorize the data right now, it would result in too many separate files. It'll be better to just do this during the beginning of the modeling phase of the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import nltk.sentiment.vader as vd\n",
    "from textblob import TextBlob\n",
    "from wordcloud import WordCloud, STOPWORDS \n",
    "from IPython.display import Image\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/Users/jasonzhou/Documents/GitHub/NintendoTweets/Documents/Capstone3\"\n",
    "os.chdir(path)\n",
    "\n",
    "NintendoTweets = pd.read_json(\"NintendoTweets.json\", lines=True,\n",
    "                        orient='columns')\n",
    "smashdata = pd.read_csv('smashdata.csv')\n",
    "firedata = pd.read_csv('firedata.csv')\n",
    "partydata = pd.read_csv('partydata.csv')\n",
    "\n",
    "NintendoTweets = NintendoTweets['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "originaltexts = [NintendoTweetsText for NintendoTweetsText in NintendoTweetsTexts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of Total/Unique Tweets Per Game\n",
      " \n",
      "Super Smash Bros. Ultimate:  12535 / 3016\n",
      "Fire Emblem: Three Houses:  1563 / 200\n",
      "Super Mario Party:  882 / 264\n"
     ]
    }
   ],
   "source": [
    "print(\"Amount of Total/Unique Tweets Per Game\")\n",
    "print(\" \")\n",
    "print(\"Super Smash Bros. Ultimate: \", len(smashdata), \"/\", len(set(smashdata['cleanedtext'])))\n",
    "print(\"Fire Emblem: Three Houses: \", len(firedata), \"/\", len(set(firedata['cleanedtext'])))\n",
    "print(\"Super Mario Party: \", len(partydata), \"/\", len(set(partydata['cleanedtext'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labeling all the data\n",
    "\n",
    "smashlabels = []\n",
    "firelabels = []\n",
    "partylabels = []\n",
    "\n",
    "for i in range(len(smashdata)):\n",
    "    blob = TextBlob(smashdata['cleanedtext'][i])\n",
    "    if blob.sentiment.polarity >= 0:\n",
    "        smashlabels.append(0)\n",
    "    else:\n",
    "        smashlabels.append(1)\n",
    "        \n",
    "for i in range(len(firedata)):\n",
    "    blob = TextBlob(firedata['cleanedtext'][i])\n",
    "    if blob.sentiment.polarity >= 0:\n",
    "        firelabels.append(0)\n",
    "    else:\n",
    "        firelabels.append(1)\n",
    "        \n",
    "for i in range(len(partydata)):\n",
    "    blob = TextBlob(partydata['cleanedtext'][i])\n",
    "    if blob.sentiment.polarity >= 0:\n",
    "        partylabels.append(0)\n",
    "    else:\n",
    "        partylabels.append(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see just what the proportion of positive to negative labels there are in our label columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    11861\n",
       "1      674\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(smashlabels).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1534\n",
       "1      29\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(firelabels).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    829\n",
       "1     53\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(partylabels).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To summarize each ratio of positive/negative tweets:\n",
    "\n",
    "|        |Game1 |Game2 |Game3 |\n",
    "|--------|------|------|------|\n",
    "|Positive|94.6% |98.1% |94.0% |\n",
    "|Negative|5.4%  |1.9%  |6.0%  |\n",
    "\n",
    "\n",
    "\n",
    "When taking samples, I want to see the original, uncleaned tweets because some do not make much sense after being cleaned. I'll also keep the cleaned versions next to their originals in the samples datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'NintendoTweets' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-4eb4e2785a08>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msmashoriginaltweets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXte1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0msmashoriginaltweets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNintendoTweets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mXte1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Unnamed: 0'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0msmashsampletweets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msmashoriginaltweets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXte1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'cleanedtext'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'NintendoTweets' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "Xtr1, Xte1, ytr1, yte1 = train_test_split(smashdata, smashlabels, test_size=0.03, random_state=1, stratify = smashlabels)\n",
    "\n",
    "smashoriginaltweets = []\n",
    "for i in range(len(Xte1)):\n",
    "    smashoriginaltweets.append(NintendoTweets[Xte1['Unnamed: 0'].iloc[i]])\n",
    "    \n",
    "smashsampletweets = list(zip(smashoriginaltweets, Xte1['cleanedtext']))\n",
    "smashsamples = pd.DataFrame(smashsampletweets, columns=['Tweet', 'cleanedtext'])\n",
    "smashsamples.to_csv(\"smashsamples.csv\")\n",
    "\n",
    "Xtr1['label'] = ytr1\n",
    "Xte1['label'] = yte1\n",
    "\n",
    "output1 = pd.concat([Xtr1, Xte1])\n",
    "output1 = output1[['cleanedtext', 'label']]\n",
    "output1.to_csv(\"smashtraining.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  if sys.path[0] == '':\n"
     ]
    }
   ],
   "source": [
    "Xtr2, Xte2, ytr2, yte2 = train_test_split(firedata, firelabels, test_size=0.15, random_state=1, stratify = firelabels)\n",
    "\n",
    "fireoriginaltweets = []\n",
    "for i in range(len(Xte2)):\n",
    "    fireoriginaltweets.append(NintendoTweets[Xte2['Unnamed: 0'].iloc[i]])\n",
    "    \n",
    "firesampletweets = list(zip(fireoriginaltweets, Xte2['cleanedtext']))\n",
    "firesamples = pd.DataFrame(firesampletweets, columns=['Tweet', 'cleanedtext'])\n",
    "firesamples.to_csv(\"firesamples.csv\")\n",
    "\n",
    "Xtr2['label'] = ytr2\n",
    "Xte2['label'] = yte2\n",
    "\n",
    "output2 = pd.concat([Xtr2, Xte2])\n",
    "output2 = output2[['cleanedtext', 'label']]\n",
    "output2.to_csv(\"firetraining.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  if sys.path[0] == '':\n"
     ]
    }
   ],
   "source": [
    "Xtr3, Xte3, ytr3, yte3 = train_test_split(partydata, partylabels, test_size=0.15, random_state=1, stratify = partylabels)\n",
    "\n",
    "partyoriginaltweets = []\n",
    "for i in range(len(Xte3)):\n",
    "    partyoriginaltweets.append(NintendoTweets[Xte3['Unnamed: 0'].iloc[i]])\n",
    "    \n",
    "partysampletweets = list(zip(partyoriginaltweets, Xte3['cleanedtext']))\n",
    "partysamples = pd.DataFrame(partysampletweets, columns=['Tweet', 'cleanedtext'])\n",
    "partysamples.to_csv(\"partysamples.csv\")\n",
    "\n",
    "Xtr3['label'] = ytr3\n",
    "Xte3['label'] = yte3\n",
    "\n",
    "output3 = pd.concat([Xtr3, Xte3])\n",
    "output3 = output1[['cleanedtext', 'label']]\n",
    "output3.to_csv(\"partytraining.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Samples have been set aside for each game and exported as a CSV. Between now and the next phase of the project, I will have had all the tweets in these samples manually labeled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
