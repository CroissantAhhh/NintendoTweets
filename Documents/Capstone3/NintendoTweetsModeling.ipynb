{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nintendo Tweets Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import datetime\n",
    "import pprint\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/Users/jasonzhou/Documents/GitHub/NintendoTweets/Documents/Capstone3\"\n",
    "os.chdir(path)\n",
    "\n",
    "smashtraining = pd.read_csv('smashtraining.csv')\n",
    "firetraining = pd.read_csv('firetraining.csv')\n",
    "partytraining = pd.read_csv('partytraining.csv')\n",
    "\n",
    "smashsamples = pd.read_csv('smashsamples.csv')\n",
    "firesamples = pd.read_csv('firesamples.csv')\n",
    "partysamples = pd.read_csv('partysamples.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeCountVec(df):\n",
    "    vectorizer = CountVectorizer(min_df=0)\n",
    "    vectorizer.fit(df['cleanedtext'])\n",
    "    array = vectorizer.transform(df['cleanedtext'])\n",
    "    array = array.toarray()\n",
    "    features = vectorizer.get_feature_names()\n",
    "    return array, features, vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "smashX, smashfeatures, smashvectorizer = makeCountVec(smashtraining)\n",
    "smashy = smashtraining['label']\n",
    "\n",
    "fireX, firefeatures, firevectorizer, = makeCountVec(firetraining)\n",
    "firey = firetraining['label']\n",
    "\n",
    "partyX, partyfeatures, partyvectorizer = makeCountVec(partytraining)\n",
    "partyy = partytraining['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here what we need to do is remove the rows corresponding to the manually labeled samples we will use to ultimately validate the models on the accuracy of TextBlob. Because of our work during the preprocessing phase of the project, I just need to drop the last n rows of each X matrix, with n corresponding to the length of each sample.\n",
    "\n",
    "The reason we do this after the vectorizing instead of before is so that the vectorizers are able to fit to every token in the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "smashX = smashX[:-len(smashsamples),]\n",
    "fireX = fireX[:-len(firesamples), :]\n",
    "partyX = partyX[:-len(partysamples), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to do the same for the y vectors too\n",
    "\n",
    "smashy = smashy[:-len(smashsamples)]\n",
    "firey = firey[:-len(firesamples)]\n",
    "partyy = partyy[:-len(partysamples)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For reference, each game will be mapped to a number, in the interest of variable name lengths:\n",
    "\n",
    "- Smash Bros. Ultimate: 1\n",
    "\n",
    "- Fire Emblem: Three Houses: 2\n",
    "\n",
    "- Super Mario Party: 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "Xtr1, Xte1, ytr1, yte1 = train_test_split(smashX, smashy, test_size=0.3, random_state=1, stratify = smashy)\n",
    "Xtr2, Xte2, ytr2, yte2 = train_test_split(fireX, firey, test_size=0.3, random_state=1, stratify = firey)\n",
    "Xtr3, Xte3, ytr3, yte3 = train_test_split(partyX, partyy, test_size=0.3, random_state=1, stratify = partyy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have training and testing data for each of our collection of tweets, I'm going to be looking at the baseline performances of four models on each of our data sets. The four models are the following:\n",
    "\n",
    "- Logistic Regression\n",
    "- Random Forest Classifier\n",
    "- XGBClassifier\n",
    "- SVC\n",
    "\n",
    "I'm going to determine the performance of each of these models using a baseline version of each. Each different type of model will be tested on each of our sets of data, which means we are going to end up with 12 sets of results in the end, that will be compiled into a table. \n",
    "\n",
    "Below defined are helper functions that will help not only to create baseline models but to determine their performance by printing out classification reports and confusion matrices. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.ticker import IndexLocator\n",
    "import itertools\n",
    "\n",
    "def plot_cm(y_test,y_pred_class,classes=['NON-default','DEFAULT']):\n",
    "    # plot confusion matrix\n",
    "    fig, ax = plt.subplots()\n",
    "    cm = confusion_matrix(y_test, y_pred_class)\n",
    "    \n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    ax.set(yticks=[-0.5, 1.5], \n",
    "           xticks=[0, 1], \n",
    "           yticklabels=classes, \n",
    "           xticklabels=classes)\n",
    "    ax.yaxis.set_major_locator(IndexLocator(base=1, offset=0.5))\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], 'd'),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    \n",
    "def scores(y_test, y_pred_class):\n",
    "    # Prints formatted classification metrics. \n",
    "    print('Classification Accuracy: ', format(accuracy_score(y_test, y_pred_class), '.3f'))\n",
    "    print('Precision score: ', format(precision_score(y_test, y_pred_class), '.3f'))\n",
    "    print('Recall score: ', format(recall_score(y_test, y_pred_class), '.3f'))\n",
    "    print('F1 score: ', format(f1_score(y_test, y_pred_class), '.3f'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "def logiRegr(X_train, y_train, X_test, y_test,**kwargs):\n",
    "    # Instantiate model. Use kwargs to pass parameters.\n",
    "    # Pass GridSearch best_params with ** to unpack.\n",
    "    logreg = LogisticRegression(random_state=1,**kwargs)\n",
    "    # Fit to training data.\n",
    "    logreg.fit(X_train, y_train)\n",
    "    # Examine coefficients\n",
    "    #pprint.pprint(list(zip(X_train.columns,logreg.coef_[0])))\n",
    "    # Class predictions (not predicted probabilities)\n",
    "    y_pred_class = logreg.predict(X_test)\n",
    "    # Scoring metrics\n",
    "    scores(y_test, y_pred_class)\n",
    "    # Plot confusion matrix\n",
    "    #plot_cm(y_test,y_pred_class)\n",
    "    \n",
    "def randomForest(X_train, y_train, X_test, y_test,**kwargs):\n",
    "    # Instantiate model. Use kwargs to pass parameters.\n",
    "    # Pass GridSearch best_params with ** to unpack.\n",
    "    rf = RandomForestClassifier(random_state=1, **kwargs) \n",
    "    # Fit to training data.\n",
    "    rf.fit(X_train,y_train)\n",
    "    # Class predictions\n",
    "    y_pred_class = rf.predict(X_test)\n",
    "    # Scoring metrics\n",
    "    scores(y_test, y_pred_class)\n",
    "    # Confusion matrix\n",
    "    #plot_cm(y_test,y_pred_class)\n",
    "    \n",
    "def xgbClass(X_train, y_train, X_test, y_test,**kwargs):\n",
    "    # Instantiate model. Use kwargs to pass parameters.\n",
    "    # Pass GridSearch best_params with ** to unpack.\n",
    "    xg = xgb.XGBClassifier(seed=1,**kwargs)\n",
    "    # Fit to training data.\n",
    "    xg.fit(X_train,y_train)\n",
    "    # Class predictions\n",
    "    y_pred_class = xg.predict(X_test)\n",
    "    # Scoring metrics\n",
    "    scores(y_test, y_pred_class)\n",
    "    # Confusion matrix\n",
    "    #plot_cm(y_test,y_pred_class)\n",
    "    \n",
    "def svmClass(X_train, y_train, X_test, y_test, **kwargs):\n",
    "    # Instantiate model. Use kwargs to pass parameters.\n",
    "    # Pass GridSearch best_params with ** to unpack.\n",
    "    svm = SVC(random_state=1,**kwargs)\n",
    "    # Fit to training data.\n",
    "    svm.fit(X_train, y_train)\n",
    "    # Class predictions\n",
    "    y_pred_class = svm.predict(X_test)\n",
    "    # Scoring metrics\n",
    "    scores(y_test, y_pred_class)\n",
    "    # Plot confusion matrix\n",
    "    #plot_cm(y_test,y_pred_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we're going to be creating and evaluating baseline models for each game. 4 types of models for 3 games, feel free to skip the lengthy amount of output, there will be a table at the end that summarizes the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Accuracy:  0.982\n",
      "Precision score:  0.986\n",
      "Recall score:  0.996\n",
      "F1 score:  0.991\n"
     ]
    }
   ],
   "source": [
    "logiRegr(Xtr1, ytr1, Xte1, yte1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Accuracy:  0.983\n",
      "Precision score:  0.990\n",
      "Recall score:  0.992\n",
      "F1 score:  0.991\n"
     ]
    }
   ],
   "source": [
    "randomForest(Xtr1, ytr1, Xte1, yte1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Accuracy:  0.982\n",
      "Precision score:  0.988\n",
      "Recall score:  0.993\n",
      "F1 score:  0.990\n"
     ]
    }
   ],
   "source": [
    "xgbClass(Xtr1, ytr1, Xte1, yte1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Accuracy:  0.983\n",
      "Precision score:  0.987\n",
      "Recall score:  0.996\n",
      "F1 score:  0.991\n"
     ]
    }
   ],
   "source": [
    "svmClass(Xtr1, ytr1, Xte1, yte1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Accuracy:  0.983\n",
      "Precision score:  0.983\n",
      "Recall score:  1.000\n",
      "F1 score:  0.992\n"
     ]
    }
   ],
   "source": [
    "logiRegr(Xtr2, ytr2, Xte2, yte2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Accuracy:  0.986\n",
      "Precision score:  0.990\n",
      "Recall score:  0.995\n",
      "F1 score:  0.993\n"
     ]
    }
   ],
   "source": [
    "randomForest(Xtr2, ytr2, Xte2, yte2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Accuracy:  0.991\n",
      "Precision score:  0.995\n",
      "Recall score:  0.995\n",
      "F1 score:  0.995\n"
     ]
    }
   ],
   "source": [
    "xgbClass(Xtr2, ytr2, Xte2, yte2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Accuracy:  0.983\n",
      "Precision score:  0.983\n",
      "Recall score:  1.000\n",
      "F1 score:  0.992\n"
     ]
    }
   ],
   "source": [
    "svmClass(Xtr2, ytr2, Xte2, yte2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Accuracy:  0.981\n",
      "Precision score:  0.985\n",
      "Recall score:  0.995\n",
      "F1 score:  0.990\n"
     ]
    }
   ],
   "source": [
    "logiRegr(Xtr3, ytr3, Xte3, yte3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Accuracy:  0.984\n",
      "Precision score:  0.992\n",
      "Recall score:  0.991\n",
      "F1 score:  0.992\n"
     ]
    }
   ],
   "source": [
    "randomForest(Xtr3, ytr3, Xte3, yte3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Accuracy:  0.982\n",
      "Precision score:  0.988\n",
      "Recall score:  0.993\n",
      "F1 score:  0.991\n"
     ]
    }
   ],
   "source": [
    "xgbClass(Xtr3, ytr3, Xte3, yte3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Accuracy:  0.982\n",
      "Precision score:  0.986\n",
      "Recall score:  0.995\n",
      "F1 score:  0.991\n"
     ]
    }
   ],
   "source": [
    "svmClass(Xtr3, ytr3, Xte3, yte3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary of Classification Accuracies\n",
    "\n",
    "Game1: Smash Bros Ultimate\n",
    "\n",
    "Game2: Fire Emblem: Three Houses\n",
    "\n",
    "Game3: Super Mario Party\n",
    "\n",
    "\n",
    "|      |Game1 |Game2 |Game3 |\n",
    "|------|------|------|------|\n",
    "|LogReg|0.982 |0.983 |0.981 |\n",
    "|RanFor|0.983 |0.986 |0.984 |\n",
    "|XGB   |0.982 |0.991 |0.982 |\n",
    "|SVM   |0.983 |0.983 |0.982 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printGridResult(grid_result):\n",
    "    print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "    means = grid_result.cv_results_['mean_test_score']\n",
    "    stds = grid_result.cv_results_['std_test_score']\n",
    "    params = grid_result.cv_results_['params']\n",
    "    for mean, stdev, param in zip(means, stds, params):\n",
    "        print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WARNING: Long walls of outputs incoming. 12 sets of hyperparameter tuning performed, one for each model for each game (4x3). Results summarized at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.986237 using {'C': 100, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "0.986121 (0.002538) with: {'C': 100, 'penalty': 'l2', 'solver': 'newton-cg'}\n",
      "0.986121 (0.002538) with: {'C': 100, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "0.986237 (0.002527) with: {'C': 100, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "0.984376 (0.002980) with: {'C': 10, 'penalty': 'l2', 'solver': 'newton-cg'}\n",
      "0.984376 (0.002980) with: {'C': 10, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "0.984415 (0.002944) with: {'C': 10, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "0.978716 (0.002111) with: {'C': 1.0, 'penalty': 'l2', 'solver': 'newton-cg'}\n",
      "0.978716 (0.002111) with: {'C': 1.0, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "0.978832 (0.002079) with: {'C': 1.0, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "0.963984 (0.002585) with: {'C': 0.1, 'penalty': 'l2', 'solver': 'newton-cg'}\n",
      "0.963984 (0.002585) with: {'C': 0.1, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "0.965031 (0.002775) with: {'C': 0.1, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "0.946150 (0.000270) with: {'C': 0.01, 'penalty': 'l2', 'solver': 'newton-cg'}\n",
      "0.946150 (0.000270) with: {'C': 0.01, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "0.946150 (0.000270) with: {'C': 0.01, 'penalty': 'l2', 'solver': 'liblinear'}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "solvers = ['newton-cg', 'lbfgs', 'liblinear']\n",
    "penalty = ['l2']\n",
    "c_values = [100, 10, 1.0, 0.1, 0.01]\n",
    "\n",
    "grid = dict(solver=solvers,penalty=penalty,C=c_values)\n",
    "model = LogisticRegression()\n",
    "\n",
    "cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=1)\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring='accuracy',error_score=0)\n",
    "grid_result = grid_search.fit(Xtr1, ytr1)\n",
    "printGridResult(grid_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.987132 using {'C': 100, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "0.987130 (0.004855) with: {'C': 100, 'penalty': 'l2', 'solver': 'newton-cg'}\n",
      "0.987130 (0.004855) with: {'C': 100, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "0.987132 (0.005820) with: {'C': 100, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "0.985775 (0.003797) with: {'C': 10, 'penalty': 'l2', 'solver': 'newton-cg'}\n",
      "0.985775 (0.003797) with: {'C': 10, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "0.986453 (0.004000) with: {'C': 10, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "0.983067 (0.004778) with: {'C': 1.0, 'penalty': 'l2', 'solver': 'newton-cg'}\n",
      "0.983067 (0.004778) with: {'C': 1.0, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "0.985436 (0.003642) with: {'C': 1.0, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "0.981710 (0.002468) with: {'C': 0.1, 'penalty': 'l2', 'solver': 'newton-cg'}\n",
      "0.981710 (0.002468) with: {'C': 0.1, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "0.981710 (0.002468) with: {'C': 0.1, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "0.981710 (0.002468) with: {'C': 0.01, 'penalty': 'l2', 'solver': 'newton-cg'}\n",
      "0.981710 (0.002468) with: {'C': 0.01, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "0.981710 (0.002468) with: {'C': 0.01, 'penalty': 'l2', 'solver': 'liblinear'}\n"
     ]
    }
   ],
   "source": [
    "grid_result = grid_search.fit(Xtr2, ytr2)\n",
    "printGridResult(grid_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.985996 using {'C': 100, 'penalty': 'l2', 'solver': 'newton-cg'}\n",
      "0.985996 (0.002284) with: {'C': 100, 'penalty': 'l2', 'solver': 'newton-cg'}\n",
      "0.985996 (0.002284) with: {'C': 100, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "0.985958 (0.002304) with: {'C': 100, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "0.985002 (0.002136) with: {'C': 10, 'penalty': 'l2', 'solver': 'newton-cg'}\n",
      "0.985002 (0.002136) with: {'C': 10, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "0.985116 (0.002172) with: {'C': 10, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "0.979607 (0.002522) with: {'C': 1.0, 'penalty': 'l2', 'solver': 'newton-cg'}\n",
      "0.979607 (0.002522) with: {'C': 1.0, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "0.979683 (0.002478) with: {'C': 1.0, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "0.965526 (0.002802) with: {'C': 0.1, 'penalty': 'l2', 'solver': 'newton-cg'}\n",
      "0.965526 (0.002802) with: {'C': 0.1, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "0.966062 (0.002481) with: {'C': 0.1, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "0.946319 (0.000279) with: {'C': 0.01, 'penalty': 'l2', 'solver': 'newton-cg'}\n",
      "0.946319 (0.000279) with: {'C': 0.01, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "0.946396 (0.000401) with: {'C': 0.01, 'penalty': 'l2', 'solver': 'liblinear'}\n"
     ]
    }
   ],
   "source": [
    "grid_result = grid_search.fit(Xtr3, ytr3)\n",
    "printGridResult(grid_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier()\n",
    "n_estimators = [10, 100, 1000]\n",
    "max_features = [None, 'sqrt', 'log2']\n",
    "grid = dict(n_estimators=n_estimators,max_features=max_features)\n",
    "\n",
    "cv = RepeatedStratifiedKFold(n_splits=3, n_repeats=3, random_state=1)\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring='accuracy',error_score=0)\n",
    "grid_result = grid_search.fit(Xtr1, ytr1)\n",
    "printGridResult(grid_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_result = grid_search.fit(Xtr2, ytr2)\n",
    "printGridResult(grid_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_result = grid_search.fit(Xtr3, ytr3)\n",
    "printGridResult(grid_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = xgb.XGBClassifier()\n",
    "n_estimators = [10, 100, 1000]\n",
    "learning_rate = [0.001, 0.01, 0.1]\n",
    "# define grid search\n",
    "grid = dict(learning_rate=learning_rate, n_estimators=n_estimators)\n",
    "cv = RepeatedStratifiedKFold(n_splits=3, n_repeats=3, random_state=1)\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=1, cv=cv, scoring='accuracy',error_score=0)\n",
    "grid_result = grid_search.fit(Xtr1, ytr1)\n",
    "printGridResult(grid_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_result = grid_search.fit(Xtr2, ytr2)\n",
    "printGridResult(grid_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_result = grid_search.fit(Xtr3, ytr3)\n",
    "printGridResult(grid_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SVC()\n",
    "kernel = ['poly', 'rbf', 'sigmoid']\n",
    "C = [50, 10, 1.0, 0.1, 0.01]\n",
    "gamma = ['scale']\n",
    "# define grid search\n",
    "grid = dict(kernel=kernel,C=C,gamma=gamma)\n",
    "cv = RepeatedStratifiedKFold(n_splits=3, n_repeats=3, random_state=1)\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=1, cv=cv, scoring='accuracy',error_score=0)\n",
    "grid_result = grid_search.fit(Xtr1, ytr1)\n",
    "printGridResult(grid_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_result = grid_search.fit(Xtr2, ytr2)\n",
    "printGridResult(grid_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_result = grid_search.fit(Xtr3, ytr3)\n",
    "printGridResult(grid_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking the best hyperparameters for each model per game, our classification accuracy scores are now the following:\n",
    "\n",
    "|      |Game1 |Game2 |Game3 |\n",
    "|------|------|------|------|\n",
    "|LogReg|0.986 |0.987 |0.962 |\n",
    "|RanFor|0.985 |0.988 |0.960 |\n",
    "|XGB   |0.979 |0.982 |0.957 |\n",
    "|SVM   |0.983 |0.985 |0.960 |\n",
    "\n",
    "\n",
    "Table of baseline model scores redepicted below:\n",
    "\n",
    "|      |Game1 |Game2 |Game3 |\n",
    "|------|------|------|------|\n",
    "|LogReg|0.982 |0.983 |0.945 |\n",
    "|RanFor|0.984 |0.981 |0.950 |\n",
    "|XGB   |0.982 |0.991 |0.950 |\n",
    "|SVM   |0.983 |0.983 |0.945 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Across the models we've built, it's now time to select the model that scores highest per game. \n",
    "\n",
    "- Game 1 (Super Smash Bros Ultimate): Logistic Regression with hyperparameters tuned\n",
    "- Game 2 (Fire Emblem: Three Houses): Random Forest with hyperparameters tuned\n",
    "- Game 3 (Super Mario Party): Logistic Regression with hyperparameters tuned\n",
    "\n",
    "\n",
    "Let's build each one with the optimal hyperparameters, starting with Smash Bros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Smash Bro's best performing model, with hyperparameters tuned\n",
    "\n",
    "smashmodel = LogisticRegression(C=100, penalty='l2',solver='liblinear')\n",
    "smashmodel.fit(Xtr1, ytr1)\n",
    "\n",
    "ypred = smashmodel.predict(Xte1)\n",
    "scores(yte1, ypred)\n",
    "plot_cm(yte1,ypred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fire Emblem's best performing model, with hyperparameters tuned\n",
    "\n",
    "firemodel = RandomForestClassifier(max_features=None, n_estimators=10)\n",
    "firemodel.fit(Xtr2, ytr2)\n",
    "\n",
    "ypred = firemodel.predict(Xte2)\n",
    "scores(yte2, ypred)\n",
    "plot_cm(yte2,ypred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mario Party's best performing model, with hyperparameters tuned\n",
    "\n",
    "partymodel = LogisticRegression(C=100, penalty='l2', solver='newton-cg')\n",
    "partymodel.fit(Xtr3, ytr3)\n",
    "\n",
    "ypred = partymodel.predict(Xte3)\n",
    "scores(yte3, ypred)\n",
    "plot_cm(yte3,ypred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeCountVecSample(df):\n",
    "    vectorizer = CountVectorizer(min_df=0)\n",
    "    vectorizer.fit(df['Cleaned'])\n",
    "    array = vectorizer.transform(df['Cleaned'])\n",
    "    array = array.toarray()\n",
    "    return array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smashsampletest = makeCountVecSample(smashsamples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smashsampletest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
