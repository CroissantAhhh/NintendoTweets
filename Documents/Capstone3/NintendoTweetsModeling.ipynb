{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nintendo Tweets Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've arrived at the modeling phase of the project. As a recap, our data is imbalanced with positively labeled tweets being the large majority and negative tweets being the minority. As such, it is important that our models are able to accurately detect negative tweets, even if it comes at the expense of accurately detecting positive tweets. The primary performance metric we will be looking at is recall, as it tells us how many negative tweets the model is able to correctly label.\n",
    "\n",
    "Because this is a binary classification problem, we will be building a variety of classification models suited for this kind of problem. I will be working with LogisticRegression, RandomForestClassifier, XGBClassifier, and SVC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import statistics\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import datetime\n",
    "import pprint\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading in files\n",
    "\n",
    "path = \"/Users/jasonzhou/Documents/GitHub/NintendoTweets/Documents/Capstone3\"\n",
    "os.chdir(path)\n",
    "\n",
    "smashtraining = pd.read_csv('smashtraining.csv')\n",
    "firetraining = pd.read_csv('firetraining.csv')\n",
    "partytraining = pd.read_csv('partytraining.csv')\n",
    "\n",
    "smashsamples = pd.read_csv('smashsamples.csv')\n",
    "firesamples = pd.read_csv('firesamples.csv')\n",
    "partysamples = pd.read_csv('partysamples.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that creates count vectorizers of corpus, and returns counts array, feature names, \n",
    "# and the vectorizer itself\n",
    "\n",
    "def makeCountVec(df):\n",
    "    vectorizer = CountVectorizer(min_df=0)\n",
    "    vectorizer.fit(df['cleanedtext'])\n",
    "    array = vectorizer.transform(df['cleanedtext'])\n",
    "    array = array.toarray()\n",
    "    features = vectorizer.get_feature_names()\n",
    "    return array, features, vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "smashX, smashfeatures, smashvectorizer = makeCountVec(smashtraining)\n",
    "smashy = smashtraining['label']\n",
    "\n",
    "fireX, firefeatures, firevectorizer, = makeCountVec(firetraining)\n",
    "firey = firetraining['label']\n",
    "\n",
    "partyX, partyfeatures, partyvectorizer = makeCountVec(partytraining)\n",
    "partyy = partytraining['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here what we need to do is remove the rows corresponding to the manually labeled samples we will use to ultimately validate the models on the accuracy of TextBlob. Because of our work during the preprocessing phase of the project, I just need to drop the last n rows of each X matrix, with n corresponding to the length of each sample.\n",
    "\n",
    "The reason we do this after the vectorizing instead of before is so that the vectorizers are able to fit to every token in the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "smashX = smashX[:-len(smashsamples),]\n",
    "fireX = fireX[:-len(firesamples), :]\n",
    "partyX = partyX[:-len(partysamples), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to do the same for the y vectors too\n",
    "\n",
    "smashy = smashy[:-len(smashsamples)]\n",
    "firey = firey[:-len(firesamples)]\n",
    "partyy = partyy[:-len(partysamples)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For reference, each game will be mapped to a number, in the interest of variable name lengths:\n",
    "\n",
    "- Smash Bros. Ultimate: 1\n",
    "\n",
    "- Fire Emblem: Three Houses: 2\n",
    "\n",
    "- Super Mario Party: 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "Xtr1, Xte1, ytr1, yte1 = train_test_split(smashX, smashy, test_size=0.3, random_state=1, stratify = smashy)\n",
    "Xtr2, Xte2, ytr2, yte2 = train_test_split(fireX, firey, test_size=0.3, random_state=1, stratify = firey)\n",
    "Xtr3, Xte3, ytr3, yte3 = train_test_split(partyX, partyy, test_size=0.3, random_state=1, stratify = partyy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have training and testing data for each of our collection of tweets, I'm going to be looking at the baseline performances of four models on each of our data sets. The four models are the following:\n",
    "\n",
    "- Logistic Regression\n",
    "- Random Forest Classifier\n",
    "- XGBClassifier\n",
    "- SVC\n",
    "\n",
    "I'm going to determine the performance of each of these models using a baseline version of each. Each different type of model will be tested on each of our sets of data, which means we are going to end up with 12 sets of results in the end, that will be compiled into a table. \n",
    "\n",
    "Below defined are helper functions that will help not only to create baseline models but to determine their performance by printing out classification reports and confusion matrices. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.ticker import IndexLocator\n",
    "import itertools\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def plot_cm(y_test,y_pred_class,classes=['NON-default','DEFAULT']):\n",
    "    # plot confusion matrix\n",
    "    fig, ax = plt.subplots()\n",
    "    cm = confusion_matrix(y_test, y_pred_class)\n",
    "    \n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    ax.set(yticks=[-0.5, 1.5], \n",
    "           xticks=[0, 1], \n",
    "           yticklabels=classes, \n",
    "           xticklabels=classes)\n",
    "    ax.yaxis.set_major_locator(IndexLocator(base=1, offset=0.5))\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], 'd'),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    \n",
    "def scores(y_test, y_pred_class):\n",
    "    # Prints formatted classification metrics. \n",
    "    #print('Classification Accuracy: ', format(accuracy_score(y_test, y_pred_class), '.3f'))\n",
    "    #print('Precision score: ', format(precision_score(y_test, y_pred_class), '.3f'))\n",
    "    #print('Recall score: ', format(recall_score(y_test, y_pred_class), '.3f'))\n",
    "    #print('F1 score: ', format(f1_score(y_test, y_pred_class), '.3f'))\n",
    "    \n",
    "    target_names = ['positive tweets', 'negative tweets']\n",
    "    print(classification_report(y_test, y_pred_class, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "def logiRegr(X_train, y_train, X_test, y_test,**kwargs):\n",
    "    # Instantiate model. Use kwargs to pass parameters.\n",
    "    # Pass GridSearch best_params with ** to unpack.\n",
    "    logreg = LogisticRegression(random_state=1,**kwargs)\n",
    "    # Fit to training data.\n",
    "    logreg.fit(X_train, y_train)\n",
    "    # Examine coefficients\n",
    "    #pprint.pprint(list(zip(X_train.columns,logreg.coef_[0])))\n",
    "    # Class predictions (not predicted probabilities)\n",
    "    print(\"Performance on Training Data\")\n",
    "    y_pred_class = logreg.predict(X_train)\n",
    "    scores(y_train, y_pred_class)\n",
    "    \n",
    "    print(\"Performance on Testing Data\")\n",
    "    y_pred_class = logreg.predict(X_test)\n",
    "    scores(y_test, y_pred_class)\n",
    "    # Plot confusion matrix\n",
    "    #plot_cm(y_test,y_pred_class)\n",
    "    \n",
    "def randomForest(X_train, y_train, X_test, y_test,**kwargs):\n",
    "    # Instantiate model. Use kwargs to pass parameters.\n",
    "    # Pass GridSearch best_params with ** to unpack.\n",
    "    rf = RandomForestClassifier(random_state=1, **kwargs) \n",
    "    # Fit to training data.\n",
    "    rf.fit(X_train,y_train)\n",
    "    # Class predictions\n",
    "    y_pred_class = rf.predict(X_test)\n",
    "    # Scoring metrics\n",
    "    scores(y_test, y_pred_class)\n",
    "    # Confusion matrix\n",
    "    #plot_cm(y_test,y_pred_class)\n",
    "    \n",
    "def xgbClass(X_train, y_train, X_test, y_test,**kwargs):\n",
    "    # Instantiate model. Use kwargs to pass parameters.\n",
    "    # Pass GridSearch best_params with ** to unpack.\n",
    "    xg = xgb.XGBClassifier(seed=1,**kwargs)\n",
    "    # Fit to training data.\n",
    "    xg.fit(X_train,y_train)\n",
    "    # Class predictions\n",
    "    y_pred_class = xg.predict(X_test)\n",
    "    # Scoring metrics\n",
    "    scores(y_test, y_pred_class)\n",
    "    # Confusion matrix\n",
    "    #plot_cm(y_test,y_pred_class)\n",
    "    \n",
    "def svmClass(X_train, y_train, X_test, y_test, **kwargs):\n",
    "    # Instantiate model. Use kwargs to pass parameters.\n",
    "    # Pass GridSearch best_params with ** to unpack.\n",
    "    svm = SVC(random_state=1,**kwargs)\n",
    "    # Fit to training data.\n",
    "    svm.fit(X_train, y_train)\n",
    "    # Class predictions\n",
    "    print(\"Performance on Training Data\")\n",
    "    y_pred_class = svm.predict(X_train)\n",
    "    scores(y_train, y_pred_class)\n",
    "    \n",
    "    print(\"Performance on Testing Data\")\n",
    "    y_pred_class = svm.predict(X_test)\n",
    "    scores(y_test, y_pred_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we're going to be creating and evaluating baseline models for each game. 4 types of models for 3 games, feel free to skip the lengthy amount of output, there will be a table at the end that summarizes the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance on Training Data\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "positive tweets       0.99      1.00      1.00      8052\n",
      "negative tweets       0.98      0.85      0.91       458\n",
      "\n",
      "       accuracy                           0.99      8510\n",
      "      macro avg       0.98      0.92      0.95      8510\n",
      "   weighted avg       0.99      0.99      0.99      8510\n",
      "\n",
      "Performance on Testing Data\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "positive tweets       0.98      1.00      0.99      3452\n",
      "negative tweets       0.90      0.70      0.79       196\n",
      "\n",
      "       accuracy                           0.98      3648\n",
      "      macro avg       0.94      0.85      0.89      3648\n",
      "   weighted avg       0.98      0.98      0.98      3648\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logiRegr(Xtr1, ytr1, Xte1, yte1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      "positive tweets       0.99      0.99      0.99      3452\n",
      "negative tweets       0.88      0.82      0.85       196\n",
      "\n",
      "       accuracy                           0.98      3648\n",
      "      macro avg       0.93      0.90      0.92      3648\n",
      "   weighted avg       0.98      0.98      0.98      3648\n",
      "\n"
     ]
    }
   ],
   "source": [
    "randomForest(Xtr1, ytr1, Xte1, yte1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      "positive tweets       0.99      0.99      0.99      3452\n",
      "negative tweets       0.89      0.77      0.83       196\n",
      "\n",
      "       accuracy                           0.98      3648\n",
      "      macro avg       0.94      0.88      0.91      3648\n",
      "   weighted avg       0.98      0.98      0.98      3648\n",
      "\n"
     ]
    }
   ],
   "source": [
    "xgbClass(Xtr1, ytr1, Xte1, yte1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance on Training Data\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "positive tweets       0.99      1.00      1.00      8052\n",
      "negative tweets       0.98      0.89      0.93       458\n",
      "\n",
      "       accuracy                           0.99      8510\n",
      "      macro avg       0.99      0.94      0.96      8510\n",
      "   weighted avg       0.99      0.99      0.99      8510\n",
      "\n",
      "Performance on Testing Data\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "positive tweets       0.98      1.00      0.99      3452\n",
      "negative tweets       0.94      0.70      0.80       196\n",
      "\n",
      "       accuracy                           0.98      3648\n",
      "      macro avg       0.96      0.85      0.90      3648\n",
      "   weighted avg       0.98      0.98      0.98      3648\n",
      "\n"
     ]
    }
   ],
   "source": [
    "svmClass(Xtr1, ytr1, Xte1, yte1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance on Training Data\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "positive tweets       0.99      1.00      1.00       912\n",
      "negative tweets       1.00      0.65      0.79        17\n",
      "\n",
      "       accuracy                           0.99       929\n",
      "      macro avg       1.00      0.82      0.89       929\n",
      "   weighted avg       0.99      0.99      0.99       929\n",
      "\n",
      "Performance on Testing Data\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "positive tweets       0.99      0.99      0.99       391\n",
      "negative tweets       0.67      0.50      0.57         8\n",
      "\n",
      "       accuracy                           0.98       399\n",
      "      macro avg       0.83      0.75      0.78       399\n",
      "   weighted avg       0.98      0.98      0.98       399\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logiRegr(Xtr2, ytr2, Xte2, yte2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      "positive tweets       0.98      1.00      0.99       391\n",
      "negative tweets       1.00      0.25      0.40         8\n",
      "\n",
      "       accuracy                           0.98       399\n",
      "      macro avg       0.99      0.62      0.70       399\n",
      "   weighted avg       0.99      0.98      0.98       399\n",
      "\n"
     ]
    }
   ],
   "source": [
    "randomForest(Xtr2, ytr2, Xte2, yte2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      "positive tweets       0.99      0.99      0.99       391\n",
      "negative tweets       0.67      0.50      0.57         8\n",
      "\n",
      "       accuracy                           0.98       399\n",
      "      macro avg       0.83      0.75      0.78       399\n",
      "   weighted avg       0.98      0.98      0.98       399\n",
      "\n"
     ]
    }
   ],
   "source": [
    "xgbClass(Xtr2, ytr2, Xte2, yte2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance on Training Data\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "positive tweets       1.00      1.00      1.00       912\n",
      "negative tweets       1.00      0.76      0.87        17\n",
      "\n",
      "       accuracy                           1.00       929\n",
      "      macro avg       1.00      0.88      0.93       929\n",
      "   weighted avg       1.00      1.00      1.00       929\n",
      "\n",
      "Performance on Testing Data\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "positive tweets       0.98      1.00      0.99       391\n",
      "negative tweets       1.00      0.25      0.40         8\n",
      "\n",
      "       accuracy                           0.98       399\n",
      "      macro avg       0.99      0.62      0.70       399\n",
      "   weighted avg       0.99      0.98      0.98       399\n",
      "\n"
     ]
    }
   ],
   "source": [
    "svmClass(Xtr2, ytr2, Xte2, yte2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance on Training Data\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "positive tweets       0.99      1.00      0.99       493\n",
      "negative tweets       1.00      0.77      0.87        31\n",
      "\n",
      "       accuracy                           0.99       524\n",
      "      macro avg       0.99      0.89      0.93       524\n",
      "   weighted avg       0.99      0.99      0.99       524\n",
      "\n",
      "Performance on Testing Data\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "positive tweets       0.95      1.00      0.97       211\n",
      "negative tweets       1.00      0.21      0.35        14\n",
      "\n",
      "       accuracy                           0.95       225\n",
      "      macro avg       0.98      0.61      0.66       225\n",
      "   weighted avg       0.95      0.95      0.94       225\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logiRegr(Xtr3, ytr3, Xte3, yte3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      "positive tweets       0.95      1.00      0.97       211\n",
      "negative tweets       0.67      0.14      0.24        14\n",
      "\n",
      "       accuracy                           0.94       225\n",
      "      macro avg       0.81      0.57      0.60       225\n",
      "   weighted avg       0.93      0.94      0.92       225\n",
      "\n"
     ]
    }
   ],
   "source": [
    "randomForest(Xtr3, ytr3, Xte3, yte3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      "positive tweets       0.95      0.98      0.97       211\n",
      "negative tweets       0.50      0.29      0.36        14\n",
      "\n",
      "       accuracy                           0.94       225\n",
      "      macro avg       0.73      0.63      0.67       225\n",
      "   weighted avg       0.93      0.94      0.93       225\n",
      "\n"
     ]
    }
   ],
   "source": [
    "xgbClass(Xtr3, ytr3, Xte3, yte3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance on Training Data\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "positive tweets       0.99      1.00      0.99       493\n",
      "negative tweets       1.00      0.77      0.87        31\n",
      "\n",
      "       accuracy                           0.99       524\n",
      "      macro avg       0.99      0.89      0.93       524\n",
      "   weighted avg       0.99      0.99      0.99       524\n",
      "\n",
      "Performance on Testing Data\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "positive tweets       0.94      1.00      0.97       211\n",
      "negative tweets       1.00      0.07      0.13        14\n",
      "\n",
      "       accuracy                           0.94       225\n",
      "      macro avg       0.97      0.54      0.55       225\n",
      "   weighted avg       0.95      0.94      0.92       225\n",
      "\n"
     ]
    }
   ],
   "source": [
    "svmClass(Xtr3, ytr3, Xte3, yte3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are disparities in the performance of our Logisitic Regression and SVM models when they are predicting training and testing data. This indicates overfitting, and additional hyperparameter tuning will be performed to address this.\n",
    "\n",
    "Due to the imbalanced nature of our data set, the challenge here is building a model that can more accurately identify the minority class, which in this case is 'negative tweets'. Because the models will have little issue identifying positive tweets either way, the key metric of performance here will be the recall score on negative tweets.\n",
    "\n",
    "To summarize the proportion of positive and negative labels in our training data sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    8052\n",
       "1     458\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ytr1.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    912\n",
       "1     17\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ytr2.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    493\n",
       "1     31\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ytr3.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on this information, we can assign weights to each class of each training data set, to use in tuning our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights1 = {0: 1, 1: 8052/458}\n",
    "weights2 = {0: 1, 1: 912/17}\n",
    "weights3 = {0: 1, 1: 493/31}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary of Recall Scores of Negative Tweets\n",
    "\n",
    "Game1: Smash Bros Ultimate\n",
    "\n",
    "Game2: Fire Emblem: Three Houses\n",
    "\n",
    "Game3: Super Mario Party\n",
    "\n",
    "\n",
    "|      |Game1 |Game2 |Game3 |\n",
    "|------|------|------|------|\n",
    "|LogReg|0.700 |0.500 |0.210 |\n",
    "|RanFor|0.820 |0.250 |0.140 |\n",
    "|XGB   |0.770 |0.500 |0.290 |\n",
    "|SVM   |0.700 |0.250 |0.070 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that prints results of gridsearch of hyperparameters\n",
    "\n",
    "def printGridResult(grid_result):\n",
    "    print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WARNING: Long walls of outputs incoming. 12 sets of hyperparameter tuning performed, one for each model for each game (4x3). Results summarized at the end.\n",
    "\n",
    "Note: many different variable names are defined and deleted after they're done being used to ensure that as much memory is freed up as possible, in case python's garbage collection isn't doing a good enough job."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Models Hyperparameter Tuning\n",
    "\n",
    "The baseline LogisticRegression model has default parameters of solver='lbgfs' and C=1.0. Let's test some different values for these parameters to see if there's a model with different parameters that yields a higher recall score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.932370 using {'C': 0.01, 'class_weight': {0: 1, 1: 17.580786026200872}, 'solver': 'saga'}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "solvers = ['newton-cg', 'lbfgs', 'saga']\n",
    "c_values = [100, 10, 1.0, 0.1, 0.01]\n",
    "class_weight = [None, weights1]\n",
    "\n",
    "grid = dict(solver=solvers,C=c_values, class_weight=class_weight)\n",
    "model1 = LogisticRegression()\n",
    "\n",
    "cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=1, random_state=1)\n",
    "grid_search1 = GridSearchCV(estimator=model1, param_grid=grid, n_jobs=4, cv=cv, scoring='recall',error_score=0)\n",
    "grid_result1 = grid_search1.fit(Xtr1, ytr1)\n",
    "printGridResult(grid_result1)\n",
    "del grid_result1, grid_search1, model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 1.000000 using {'C': 0.01, 'class_weight': {0: 1, 1: 53.64705882352941}, 'solver': 'newton-cg'}\n"
     ]
    }
   ],
   "source": [
    "class_weight = [None, weights2]\n",
    "\n",
    "grid = dict(solver=solvers,C=c_values, class_weight=class_weight)\n",
    "cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=1, random_state=1)\n",
    "\n",
    "model2 = LogisticRegression()\n",
    "grid_search2 = GridSearchCV(estimator=model2, param_grid=grid, n_jobs=4, cv=cv, scoring='recall',error_score=0)\n",
    "grid_result2 = grid_search2.fit(Xtr2, ytr2)\n",
    "printGridResult(grid_result2)\n",
    "del grid_search2, grid_result2, model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.904762 using {'C': 0.01, 'class_weight': {0: 1, 1: 15.903225806451612}, 'solver': 'newton-cg'}\n"
     ]
    }
   ],
   "source": [
    "class_weight = [None, weights3]\n",
    "\n",
    "grid = dict(solver=solvers,C=c_values, class_weight=class_weight)\n",
    "cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=1, random_state=1)\n",
    "\n",
    "model3 = LogisticRegression()\n",
    "grid_search3 = GridSearchCV(estimator=model3, param_grid=grid, n_jobs=4, cv=cv, scoring='recall',error_score=0)\n",
    "grid_result3 = grid_search3.fit(Xtr3, ytr3)\n",
    "printGridResult(grid_result3)\n",
    "del grid_search3, grid_result3, model3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Models Hyperparameter Tuning\n",
    "\n",
    "The baseline RandomForestClassifier has a default of n_estimators=100. More trees in general will always improve the performance of the model, so we'll test n_estimators=1000 to see if there's a significant improvement in performance. We'll also play around with the max_features parameter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.827520 using {'class_weight': None, 'max_features': 'sqrt', 'n_estimators': 100}\n"
     ]
    }
   ],
   "source": [
    "model4 = RandomForestClassifier()\n",
    "\n",
    "class_weight = [None, weights1]\n",
    "n_estimators = [100, 1000]\n",
    "max_features = ['sqrt', 'log2']\n",
    "grid = dict(class_weight=class_weight, n_estimators=n_estimators, max_features=max_features)\n",
    "cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=1, random_state=1)\n",
    "\n",
    "grid_search4 = GridSearchCV(estimator=model4, param_grid=grid, n_jobs=4, cv=cv, scoring='recall',error_score=0)\n",
    "grid_result4 = grid_search4.fit(Xtr1, ytr1)\n",
    "printGridResult(grid_result4)\n",
    "del grid_search4, grid_result4, model4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.116667 using {'class_weight': None, 'max_features': 'sqrt', 'n_estimators': 1000}\n"
     ]
    }
   ],
   "source": [
    "model5 = RandomForestClassifier()\n",
    "\n",
    "class_weight = [None, weights2]\n",
    "grid = dict(class_weight=class_weight, n_estimators=n_estimators, max_features=max_features)\n",
    "cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=1, random_state=1)\n",
    "\n",
    "grid_search5 = GridSearchCV(estimator=model5, param_grid=grid, n_jobs=4, cv=cv, scoring='recall',error_score=0)\n",
    "grid_result5 = grid_search5.fit(Xtr2, ytr2)\n",
    "printGridResult(grid_result5)\n",
    "del grid_search5, grid_result5, model5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.280952 using {'class_weight': None, 'max_features': 'sqrt', 'n_estimators': 100}\n"
     ]
    }
   ],
   "source": [
    "model6 = RandomForestClassifier()\n",
    "\n",
    "class_weight = [None, weights3]\n",
    "grid = dict(class_weight=class_weight, n_estimators=n_estimators, max_features=max_features)\n",
    "cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=1, random_state=1)\n",
    "\n",
    "grid_search6 = GridSearchCV(estimator=model6, param_grid=grid, n_jobs=4, cv=cv, scoring='recall',error_score=0)\n",
    "grid_result6 = grid_search6.fit(Xtr3, ytr3)\n",
    "printGridResult(grid_result6)\n",
    "del grid_search6, grid_result6, model6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost Models Hyperparameter Tuning\n",
    "\n",
    "The default parameters for XGBClassifier are max_depth=3, n_estimators=100, and learning_rate=0.1. We'll slightly increase max_depth while testing for different values of the other parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.984783 using {'learning_rate': 0.001, 'n_estimators': 1000, 'scale_pos_weight': 17.580786026200872}\n"
     ]
    }
   ],
   "source": [
    "model7 = xgb.XGBClassifier(max_depth=5)\n",
    "\n",
    "scale_pos_weight = [1, 8052/458]\n",
    "learning_rate = [0.001, 0.01, 0.1]\n",
    "n_estimators = [100, 1000]\n",
    "\n",
    "grid = dict(scale_pos_weight=scale_pos_weight, learning_rate=learning_rate, n_estimators=n_estimators)\n",
    "cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=1, random_state=1)\n",
    "\n",
    "grid_search7 = GridSearchCV(estimator=model7, param_grid=grid, n_jobs=4, cv=cv, scoring='recall',error_score=0)\n",
    "grid_result7 = grid_search7.fit(Xtr1, ytr1)\n",
    "printGridResult(grid_result7)\n",
    "del grid_search7, grid_result7, model7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.766667 using {'learning_rate': 0.001, 'n_estimators': 100, 'scale_pos_weight': 53.64705882352941}\n"
     ]
    }
   ],
   "source": [
    "model8 = xgb.XGBClassifier(max_depth=5)\n",
    "\n",
    "scale_pos_weight = [1, 912/17]\n",
    "grid = dict(scale_pos_weight=scale_pos_weight, learning_rate=learning_rate, n_estimators=n_estimators)\n",
    "cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=1, random_state=1)\n",
    "\n",
    "grid_search8 = GridSearchCV(estimator=model8, param_grid=grid, n_jobs=4, cv=cv, scoring='recall',error_score=0)\n",
    "grid_result8 = grid_search8.fit(Xtr2, ytr2)\n",
    "printGridResult(grid_result8)\n",
    "del grid_search8, grid_result8, model8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.904762 using {'learning_rate': 0.001, 'n_estimators': 100, 'scale_pos_weight': 15.903225806451612}\n"
     ]
    }
   ],
   "source": [
    "model9 = xgb.XGBClassifier(max_depth=5)\n",
    "\n",
    "scale_pos_weight = [1, 493/31]\n",
    "grid = dict(scale_pos_weight=scale_pos_weight, learning_rate=learning_rate, n_estimators=n_estimators)\n",
    "cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=1, random_state=1)\n",
    "\n",
    "grid_search9 = GridSearchCV(estimator=model9, param_grid=grid, n_jobs=4, cv=cv, scoring='recall',error_score=0)\n",
    "grid_result9 = grid_search9.fit(Xtr3, ytr3)\n",
    "printGridResult(grid_result9)\n",
    "del grid_search9, grid_result9, model9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM Models Hyperparameter Tuning\n",
    "\n",
    "The baseline SVC model's default parameters are kernel='rbf', C=1.0. We'll play around with different values of these parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.997826 using {'C': 0.01, 'class_weight': {0: 1, 1: 17.580786026200872}, 'kernel': 'poly'}\n"
     ]
    }
   ],
   "source": [
    "model10 = SVC()\n",
    "\n",
    "class_weight = [None, weights1]\n",
    "kernel = ['poly', 'rbf', 'sigmoid']\n",
    "C = [50, 10, 1.0, 0.1, 0.01]\n",
    "\n",
    "# define grid search\n",
    "grid = dict(class_weight=class_weight, kernel=kernel,C=C)\n",
    "cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=1, random_state=1)\n",
    "\n",
    "grid_search10 = GridSearchCV(estimator=model10, param_grid=grid, n_jobs=4, cv=cv, scoring='recall',error_score=0)\n",
    "grid_result10 = grid_search10.fit(Xtr1, ytr1)\n",
    "printGridResult(grid_result10)\n",
    "del grid_search10, grid_result10, model10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 1.000000 using {'C': 0.01, 'class_weight': {0: 1, 1: 53.64705882352941}, 'kernel': 'poly'}\n"
     ]
    }
   ],
   "source": [
    "model11 = SVC()\n",
    "\n",
    "class_weight = [None, weights2]\n",
    "\n",
    "# define grid search\n",
    "grid = dict(class_weight=class_weight, kernel=kernel,C=C)\n",
    "cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=1, random_state=1)\n",
    "grid_search11 = GridSearchCV(estimator=model11, param_grid=grid, n_jobs=4, cv=cv, scoring='recall',error_score=0)\n",
    "grid_result11 = grid_search11.fit(Xtr2, ytr2)\n",
    "printGridResult(grid_result11)\n",
    "del grid_search11, grid_result11, model11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 1.000000 using {'C': 0.01, 'class_weight': {0: 1, 1: 15.903225806451612}, 'kernel': 'poly'}\n"
     ]
    }
   ],
   "source": [
    "model12 = SVC()\n",
    "\n",
    "class_weight = [None, weights3]\n",
    "\n",
    "grid = dict(class_weight=class_weight, kernel=kernel,C=C)\n",
    "cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=1, random_state=1)\n",
    "grid_search12 = GridSearchCV(estimator=model12, param_grid=grid, n_jobs=4, cv=cv, scoring='recall',error_score=0)\n",
    "grid_result12 = grid_search12.fit(Xtr3, ytr3)\n",
    "printGridResult(grid_result12)\n",
    "del grid_search12, grid_result12, model12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking the best hyperparameters for each model per game, our recall scores are now the following:\n",
    "\n",
    "\n",
    "|      |Game1 |Game2 |Game3 |\n",
    "|------|------|------|------|\n",
    "|LogReg|0.932 |1.000 |0.905 |\n",
    "|RanFor|0.828 |0.117 |0.281 |\n",
    "|XGB   |0.985 |0.767 |0.905 |\n",
    "|SVM   |0.998 |1.000 |1.000 |\n",
    "\n",
    "Because during hyperparameter tuning, we only looked at overall recall instead of recall of only negative tweets, the results look much better than they actually are. But based off of this, we can at least understand which models perform better relatively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the hyperparameter tuning, we'll build models according to the best hyperparameters and fine tune the class weights further until the models' classification performance lands in a more desirable place. \n",
    "\n",
    "- Game 1 (Super Smash Bros Ultimate): SVM\n",
    "- Game 2 (Fire Emblem: Three Houses): LogReg, SVM\n",
    "- Game 3 (Super Mario Party): SVM\n",
    "\n",
    "\n",
    "Let's build each of these models with the optimal hyperparameters, starting with Smash Bros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extended Model Tuning - Class Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      "positive tweets       1.00      0.63      0.77      3452\n",
      "negative tweets       0.13      1.00      0.23       196\n",
      "\n",
      "       accuracy                           0.65      3648\n",
      "      macro avg       0.57      0.81      0.50      3648\n",
      "   weighted avg       0.95      0.65      0.74      3648\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "smashmodel = SVC(C=0.01, class_weight=weights1, kernel='poly')\n",
    "smashmodel.fit(Xtr1, ytr1)\n",
    "\n",
    "ypred = smashmodel.predict(Xte1)\n",
    "scores(yte1, ypred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model seems to be overly sensitive about labeling tweets as negative now. A recall of 1.00 for the negative tweets suggests that while the model was able to catch every negatively labeled tweet as negative, it did so by aggressively predicting \"negative\" at the expense of mislabeling many of the positive tweets. This is further supported by the low precision score when it comes to negative tweets. Let's perform some further adjusting on the weights to see if we can find a reasonable compromise.\n",
    "\n",
    "The weights are too \"heavy\" if they're causing our models to become too paranoid over mislabeling negative tweets. Therefore we need to lower the weights until a reasonable balance between its performance when it comes to classifying positive and negative tweets.\n",
    "\n",
    "After manual trial and error, the model performance was found to be at its most balanced when the negative tweet class weight was reduced by a factor of 1.37. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      "positive tweets       0.99      0.94      0.97      3452\n",
      "negative tweets       0.45      0.83      0.59       196\n",
      "\n",
      "       accuracy                           0.94      3648\n",
      "      macro avg       0.72      0.89      0.78      3648\n",
      "   weighted avg       0.96      0.94      0.95      3648\n",
      "\n"
     ]
    }
   ],
   "source": [
    "adjust = 1.37\n",
    "weightstest = {0:1, 1: (8052 / (452 * adjust))}\n",
    "\n",
    "smashmodel = SVC(C=0.01, class_weight=weightstest, kernel='poly')\n",
    "smashmodel.fit(Xtr1, ytr1)\n",
    "\n",
    "ypred = smashmodel.predict(Xte1)\n",
    "scores(yte1, ypred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking much better, recall for negative tweets while slightly lower is far more precise, and is no longer interfering with classification of positive tweets. On a more general note, we can also see that the overall accuracy is much higher, starting from 0.65 and ending at 0.94.\n",
    "\n",
    "Moving on to Fire Emblem's best performing model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      "positive tweets       1.00      0.85      0.92       391\n",
      "negative tweets       0.12      1.00      0.21         8\n",
      "\n",
      "       accuracy                           0.85       399\n",
      "      macro avg       0.56      0.92      0.57       399\n",
      "   weighted avg       0.98      0.85      0.90       399\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fire Emblem's best performing model, with hyperparameters tuned\n",
    "\n",
    "firemodel = LogisticRegression(C=0.01, class_weight=weights2, solver='newton-cg')\n",
    "firemodel.fit(Xtr2, ytr2)\n",
    "\n",
    "ypred = firemodel.predict(Xte2)\n",
    "scores(yte2, ypred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again with the same issue of the weights being too skewed towards labeling the negative tweets. High recall with low precision means that the model is just guessing negative more often just to be safe. While we don't mind more false positives, let's see if we can adjust the weights more to improve the precision of negative tweet predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      "positive tweets       1.00      0.98      0.99       391\n",
      " negativetweets       0.47      0.88      0.61         8\n",
      "\n",
      "       accuracy                           0.98       399\n",
      "      macro avg       0.73      0.93      0.80       399\n",
      "   weighted avg       0.99      0.98      0.98       399\n",
      "\n"
     ]
    }
   ],
   "source": [
    "adjust = 2.6\n",
    "weightstest = {0:1, 1: (912 / (17 * adjust))}\n",
    "\n",
    "firemodel = LogisticRegression(C=0.01, class_weight=weightstest, solver='newton-cg')\n",
    "firemodel.fit(Xtr2, ytr2)\n",
    "\n",
    "ypred = firemodel.predict(Xte2)\n",
    "scores(yte2, ypred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much more preferable, improved scores across the board, with only a very slight hit to the recall of negative tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      "positive tweets       1.00      0.82      0.90       391\n",
      " negativetweets       0.10      1.00      0.19         8\n",
      "\n",
      "       accuracy                           0.82       399\n",
      "      macro avg       0.55      0.91      0.54       399\n",
      "   weighted avg       0.98      0.82      0.89       399\n",
      "\n"
     ]
    }
   ],
   "source": [
    "firemodel = SVC(C=0.01, class_weight=weights2, kernel='poly')\n",
    "firemodel.fit(Xtr2, ytr2)\n",
    "\n",
    "ypred = firemodel.predict(Xte2)\n",
    "scores(yte2, ypred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      "positive tweets       0.99      0.99      0.99       391\n",
      " negativetweets       0.71      0.62      0.67         8\n",
      "\n",
      "       accuracy                           0.99       399\n",
      "      macro avg       0.85      0.81      0.83       399\n",
      "   weighted avg       0.99      0.99      0.99       399\n",
      "\n"
     ]
    }
   ],
   "source": [
    "weightstest = {0:1, 1: (912 / (17 * 2.6))}\n",
    "\n",
    "firemodel = SVC(C=0.01, class_weight=weightstest, kernel='poly')\n",
    "firemodel.fit(Xtr2, ytr2)\n",
    "\n",
    "ypred = firemodel.predict(Xte2)\n",
    "scores(yte2, ypred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=0.01, break_ties=False, cache_size=200,\n",
       "    class_weight={0: 1, 1: 11.45352956892446}, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='poly',\n",
       "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "    tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "firemodel = SVC(C=0.01, class_weight=weightstest, kernel='poly')\n",
    "firemodel.fit(Xtr2, ytr2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      "positive tweets       1.00      0.35      0.51       211\n",
      " negativetweets       0.09      1.00      0.17        14\n",
      "\n",
      "       accuracy                           0.39       225\n",
      "      macro avg       0.55      0.67      0.34       225\n",
      "   weighted avg       0.94      0.39      0.49       225\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Mario Party's best performing model\n",
    "\n",
    "partymodel = SVC(C=0.01, class_weight=weights3, kernel='poly')\n",
    "partymodel.fit(Xtr3, ytr3)\n",
    "\n",
    "ypred = partymodel.predict(Xte3)\n",
    "scores(yte3, ypred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      "positive tweets       0.98      0.95      0.96       211\n",
      " negativetweets       0.45      0.64      0.53        14\n",
      "\n",
      "       accuracy                           0.93       225\n",
      "      macro avg       0.71      0.80      0.75       225\n",
      "   weighted avg       0.94      0.93      0.93       225\n",
      "\n"
     ]
    }
   ],
   "source": [
    "weightstest = {0: 1, 1: 493 / (31 * 1.3885)}\n",
    "\n",
    "partymodel = SVC(C=0.01, class_weight=weightstest, kernel='poly')\n",
    "partymodel.fit(Xtr3, ytr3)\n",
    "\n",
    "ypred = partymodel.predict(Xte3)\n",
    "scores(yte3, ypred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tree Model Tuning - Class Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      "positive tweets       1.00      0.68      0.81      3452\n",
      "negative tweets       0.15      0.98      0.26       196\n",
      "\n",
      "       accuracy                           0.69      3648\n",
      "      macro avg       0.57      0.83      0.53      3648\n",
      "   weighted avg       0.95      0.69      0.78      3648\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# XGBClassifier for Smash Bros\n",
    "\n",
    "smashXGB = xgb.XGBClassifier(max_depth=5, learning_rate=0.001, n_estimators=1000, scale_pos_weight=(8052/458))\n",
    "smashXGB.fit(Xtr1, ytr1)\n",
    "ypred = smashXGB.predict(Xte1)\n",
    "\n",
    "scores(yte1, ypred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      "positive tweets       0.98      0.99      0.99      3452\n",
      "negative tweets       0.79      0.71      0.75       196\n",
      "\n",
      "       accuracy                           0.97      3648\n",
      "      macro avg       0.89      0.85      0.87      3648\n",
      "   weighted avg       0.97      0.97      0.97      3648\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# XGBClassifier for Smash Bros\n",
    "\n",
    "adjust=1.5\n",
    "\n",
    "smashXGB = xgb.XGBClassifier(max_depth=5, learning_rate=0.001, n_estimators=1000, scale_pos_weight=(8052/(458*adjust)))\n",
    "smashXGB.fit(Xtr1, ytr1)\n",
    "ypred = smashXGB.predict(Xte1)\n",
    "\n",
    "scores(yte1, ypred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      "positive tweets       1.00      0.90      0.95       391\n",
      "negative tweets       0.17      1.00      0.29         8\n",
      "\n",
      "       accuracy                           0.90       399\n",
      "      macro avg       0.58      0.95      0.62       399\n",
      "   weighted avg       0.98      0.90      0.93       399\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# XGBClassifier for Smash Bros\n",
    "\n",
    "fireXGB = xgb.XGBClassifier(max_depth=5, learning_rate=0.001, n_estimators=100, scale_pos_weight=(912/17))\n",
    "fireXGB.fit(Xtr2, ytr2)\n",
    "ypred = fireXGB.predict(Xte2)\n",
    "\n",
    "scores(yte2, ypred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      "positive tweets       0.99      0.99      0.99       391\n",
      "negative tweets       0.67      0.50      0.57         8\n",
      "\n",
      "       accuracy                           0.98       399\n",
      "      macro avg       0.83      0.75      0.78       399\n",
      "   weighted avg       0.98      0.98      0.98       399\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# XGBClassifier for Smash Bros\n",
    "\n",
    "adjust=10\n",
    "\n",
    "fireXGB = xgb.XGBClassifier(max_depth=5, learning_rate=0.001, n_estimators=100, scale_pos_weight=(912/(17*adjust)))\n",
    "fireXGB.fit(Xtr2, ytr2)\n",
    "ypred = fireXGB.predict(Xte2)\n",
    "\n",
    "scores(yte2, ypred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      "positive tweets       0.98      0.77      0.86       211\n",
      "negative tweets       0.17      0.71      0.28        14\n",
      "\n",
      "       accuracy                           0.77       225\n",
      "      macro avg       0.57      0.74      0.57       225\n",
      "   weighted avg       0.93      0.77      0.83       225\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# XGBClassifier for Smash Bros\n",
    "\n",
    "partyXGB = xgb.XGBClassifier(max_depth=5, learning_rate=0.001, n_estimators=100, scale_pos_weight=(493/31))\n",
    "partyXGB.fit(Xtr3, ytr3)\n",
    "ypred = partyXGB.predict(Xte3)\n",
    "\n",
    "scores(yte3, ypred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      "positive tweets       0.97      0.99      0.98       211\n",
      "negative tweets       0.73      0.57      0.64        14\n",
      "\n",
      "       accuracy                           0.96       225\n",
      "      macro avg       0.85      0.78      0.81       225\n",
      "   weighted avg       0.96      0.96      0.96       225\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# XGBClassifier for Smash Bros\n",
    "\n",
    "adjust=6\n",
    "\n",
    "partyXGB = xgb.XGBClassifier(max_depth=5, learning_rate=0.001, n_estimators=100, scale_pos_weight=(493/(31*adjust)))\n",
    "partyXGB.fit(Xtr3, ytr3)\n",
    "ypred = partyXGB.predict(Xte3)\n",
    "\n",
    "scores(yte3, ypred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parametric Model Tuning - Class Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      "positive tweets       0.99      0.90      0.94      3452\n",
      "negative tweets       0.34      0.91      0.49       196\n",
      "\n",
      "       accuracy                           0.90      3648\n",
      "      macro avg       0.67      0.90      0.72      3648\n",
      "   weighted avg       0.96      0.90      0.92      3648\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fire Emblem's best performing model, with hyperparameters tuned\n",
    "\n",
    "smashLR = LogisticRegression(C=0.01, class_weight=weights1, solver='saga')\n",
    "smashLR.fit(Xtr1, ytr1)\n",
    "\n",
    "ypred = smashLR.predict(Xte1)\n",
    "scores(yte1, ypred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      "positive tweets       0.99      0.96      0.97      3452\n",
      "negative tweets       0.52      0.83      0.64       196\n",
      "\n",
      "       accuracy                           0.95      3648\n",
      "      macro avg       0.76      0.89      0.81      3648\n",
      "   weighted avg       0.96      0.95      0.96      3648\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fire Emblem's best performing model, with hyperparameters tuned\n",
    "\n",
    "adjust=1.5\n",
    "weightstest = {0:1, 1: (8052 / (452 * adjust))}\n",
    "\n",
    "smashLR = LogisticRegression(C=0.01, class_weight=weightstest, solver='saga')\n",
    "smashLR.fit(Xtr1, ytr1)\n",
    "\n",
    "ypred = smashLR.predict(Xte1)\n",
    "scores(yte1, ypred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      "positive tweets       1.00      0.85      0.92       391\n",
      "negative tweets       0.12      1.00      0.21         8\n",
      "\n",
      "       accuracy                           0.85       399\n",
      "      macro avg       0.56      0.92      0.57       399\n",
      "   weighted avg       0.98      0.85      0.90       399\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fire Emblem's best performing model, with hyperparameters tuned\n",
    "\n",
    "fireLR = LogisticRegression(C=0.01, class_weight=weights2, solver='newton-cg')\n",
    "fireLR.fit(Xtr2, ytr2)\n",
    "\n",
    "ypred = fireLR.predict(Xte2)\n",
    "scores(yte2, ypred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      "positive tweets       1.00      0.98      0.99       391\n",
      "negative tweets       0.50      0.88      0.64         8\n",
      "\n",
      "       accuracy                           0.98       399\n",
      "      macro avg       0.75      0.93      0.81       399\n",
      "   weighted avg       0.99      0.98      0.98       399\n",
      "\n"
     ]
    }
   ],
   "source": [
    "adjust=2.69\n",
    "weightstest = {0:1, 1: (912 / (17 * adjust))}\n",
    "\n",
    "fireLR = LogisticRegression(C=0.01, class_weight=weightstest, solver='newton-cg')\n",
    "fireLR.fit(Xtr2, ytr2)\n",
    "\n",
    "ypred = fireLR.predict(Xte2)\n",
    "scores(yte2, ypred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      "positive tweets       1.00      0.69      0.81       211\n",
      "negative tweets       0.17      1.00      0.30        14\n",
      "\n",
      "       accuracy                           0.71       225\n",
      "      macro avg       0.59      0.84      0.56       225\n",
      "   weighted avg       0.95      0.71      0.78       225\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fire Emblem's best performing model, with hyperparameters tuned\n",
    "\n",
    "partyLR = LogisticRegression(C=0.01, class_weight=weights3, solver='newton-cg')\n",
    "partyLR.fit(Xtr3, ytr3)\n",
    "\n",
    "ypred = partyLR.predict(Xte3)\n",
    "scores(yte3, ypred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      "positive tweets       0.98      0.96      0.97       211\n",
      "negative tweets       0.53      0.64      0.58        14\n",
      "\n",
      "       accuracy                           0.94       225\n",
      "      macro avg       0.75      0.80      0.77       225\n",
      "   weighted avg       0.95      0.94      0.94       225\n",
      "\n"
     ]
    }
   ],
   "source": [
    "adjust=1.8\n",
    "weightstest = {0:1, 1: (493 / (31 * adjust))}\n",
    "\n",
    "partyLR = LogisticRegression(C=0.01, class_weight=weightstest, solver='newton-cg')\n",
    "partyLR.fit(Xtr3, ytr3)\n",
    "\n",
    "ypred = partyLR.predict(Xte3)\n",
    "scores(yte3, ypred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary of Adjusted Weights\n",
    "\n",
    "|      |Game1 |Game2 |Game3 |\n",
    "|------|------|------|------|\n",
    "|LogReg|1.5   |2.69  |1.8   |\n",
    "|XGB   |1.5   |10    |6     |\n",
    "|SVM   |1.37  |2.6   |1.3885|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Against Manually Labeled Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Going into this, I expect the results to be quite catastrophic actually. While labeling sample data myself, I noticed that there were very rarely tweets that conveyed clear negative sentiment. What doesn't help is when tweets often use a lot of negative words but to actually convey positive sentiment. For example, a tweet in response to Mario Party says, \"Oh God no, not Mario Party! That game destroys friendships I tell ya!\". Even a human might not initially perceive this tweet as negative without understanding the context of the game. Mario Party is a party game where players need to get themselves ahead while also sabotaging other players to keep them behind. Hence, the \"destroys friendships\" part. \n",
    "\n",
    "Let's just see what the results are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      "positive tweets       0.92      0.99      0.95       342\n",
      "negative tweets       0.50      0.11      0.19        35\n",
      "\n",
      "       accuracy                           0.91       377\n",
      "      macro avg       0.71      0.55      0.57       377\n",
      "   weighted avg       0.88      0.91      0.88       377\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluating Smash Bros model against manually labeled tweets\n",
    "\n",
    "adjust = 1.37\n",
    "weightstest = {0:1, 1: (8052 / (452 * adjust))}\n",
    "\n",
    "smashmodel = SVC(C=0.01, class_weight=weightstest, kernel='poly')\n",
    "smashmodel.fit(Xtr1, ytr1)\n",
    "\n",
    "smashsamplevec = smashvectorizer.transform(smashsamples['cleanedtext']).toarray()\n",
    "smashsamplepred = smashmodel.predict(smashsamplevec)\n",
    "scores(smashsamplepred, smashsamples['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      "positive tweets       0.96      0.98      0.97       226\n",
      "negative tweets       0.00      0.00      0.00         9\n",
      "\n",
      "       accuracy                           0.94       235\n",
      "      macro avg       0.48      0.49      0.48       235\n",
      "   weighted avg       0.92      0.94      0.93       235\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluating Fire Emblem model against manually labeled tweets\n",
    "\n",
    "adjust = 2.6\n",
    "weightstest = {0:1, 1: (912 / (17 * adjust))}\n",
    "\n",
    "firemodel = LogisticRegression(C=0.01, class_weight=weightstest, solver='newton-cg')\n",
    "firemodel.fit(Xtr2, ytr2)\n",
    "\n",
    "firesamplevec = firevectorizer.transform(firesamples['cleanedtext']).toarray()\n",
    "firesamplepred = firemodel.predict(firesamplevec)\n",
    "scores(firesamplepred, firesamples['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      "positive tweets       0.94      0.98      0.96       124\n",
      "negative tweets       0.25      0.11      0.15         9\n",
      "\n",
      "       accuracy                           0.92       133\n",
      "      macro avg       0.59      0.54      0.56       133\n",
      "   weighted avg       0.89      0.92      0.90       133\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluating Mario Party model against manually labeled tweets\n",
    "\n",
    "weightstest = {0: 1, 1: 493 / (31 * 1.3885)}\n",
    "\n",
    "partymodel = SVC(C=0.01, class_weight=weightstest, kernel='poly')\n",
    "partymodel.fit(Xtr3, ytr3)\n",
    "\n",
    "partysamplevec = partyvectorizer.transform(partysamples['cleanedtext']).toarray()\n",
    "partysamplepred = partymodel.predict(partysamplevec)\n",
    "scores(partysamplepred, partysamples['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion of Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results are rather damning, but not all that suprising. Our data is inherently very imbalanced after being labeled by TextBlob, and even more imbalanced when manually labeled. Furthermore, our approach of representing tweets using bag-of-words representations is likely too crude and loses too much information such as phrases and sentence structure. We will continue on with further experimentation with the modeling and the insights that we can draw from them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately our modeling turned out to be a failure. Due to the nature of our data, it was very important for our training data and model to be complex enough to be able to accurately detect the rare negative tweets. In other words the tolerance for our false postive rate was extremely low, for the models to be considered acceptable. The problem is that our bag-of-word representations lack the nuance to truly capture sentiment of the original tweets. Not only this, but TextBlob is most likely not an accurate labeler of our documents (tweets) here either. TextBlob's sentiment analysis is based on a model trained on movie reviews. The language used in movie reviews include many more clear postive or negative words, whereas the language used in tweets are more unfocused and include much more slang. Therefore even a bag-of-word representation of a movie review would still retain enough information to reasonably evaluate sentiment off of. Moving forward, the first step to creating models that accurately label tweets, could be to have a large enough training data set of accurately labeled tweets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
